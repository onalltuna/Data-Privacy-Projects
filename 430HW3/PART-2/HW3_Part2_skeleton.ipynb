{"cells":[{"cell_type":"markdown","metadata":{"id":"nHTPUWA64Uxo"},"source":["### Import Packages"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4297,"status":"ok","timestamp":1702463162742,"user":{"displayName":"EGEHAN ERALP","userId":"17724585025467029304"},"user_tz":-180},"id":"c7MtjE76DG5k","outputId":"22e10017-4be5-4ac8-d980-6742bc661c79"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'gensim'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggedDocument\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"]}],"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import multiprocessing\n","import random\n","\n","from gensim.models import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","import nltk\n","nltk.download('punkt')\n","nltk.download('words')\n","from nltk.corpus import words\n","\n","from sklearn import utils\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from collections import Counter\n","\n","# ----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n","# If you need any additional packages, import them down below.\n","# ----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n"]},{"cell_type":"markdown","metadata":{"id":"_GN2y1RO4X38"},"source":["## Connect to Google Drive (optional for loading data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25574,"status":"ok","timestamp":1702463189956,"user":{"displayName":"EGEHAN ERALP","userId":"17724585025467029304"},"user_tz":-180},"id":"VyEmlU9t6jMo","outputId":"154b4073-1125-407c-9857-12ef0c16626f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"pm2WMeqt130h"},"source":["# Required Functions (please do not modify these functions)"]},{"cell_type":"markdown","metadata":{"id":"feuRGHS82p07"},"source":["Functions necessary to read data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-MdLEdt2AmL"},"outputs":[],"source":["def load_train_data(path):\n","    train_dataFrame = pd.read_csv(path)\n","    return train_dataFrame\n","\n","def load_test_data(path):\n","    test_dataFrame = pd.read_csv(path)\n","    return test_dataFrame"]},{"cell_type":"markdown","metadata":{"id":"I6IUYpsz3DJe"},"source":["Preprocessing functions required for the Doc2Vec model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfB4p8_O2AoU"},"outputs":[],"source":["def tokenize_text(review):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(review):\n","        for word in nltk.word_tokenize(sent):\n","            tokens.append(word)\n","    return tokens\n","\n","def tagging_docs(dataFrame, textFeatureName = \"text\", classFeatureName = \"label\"):\n","    dataFrame[textFeatureName] = dataFrame.text.astype(str)\n","\n","    dataFrame_tagged = dataFrame.apply(\n","        lambda r: TaggedDocument(words=tokenize_text(r[textFeatureName]), tags=[r[classFeatureName]]), axis=1)\n","\n","    return dataFrame_tagged"]},{"cell_type":"markdown","metadata":{"id":"_iazj8Kv3gDU"},"source":["Functions necessary for training the Doc2Vec model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyLAtrl43gY3"},"outputs":[],"source":["def vec_for_learning(model, tagged_docs):\n","    sents = tagged_docs.values\n","    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n","    return targets, regressors\n","\n","def doc2vec_training(train_tagged, test_tagged):\n","    cores = multiprocessing.cpu_count()\n","\n","    model_dbow = Doc2Vec(dm=0 , vector_size=50, window=5, negative=5, hs=0, min_count=2, workers=multiprocessing.cpu_count())\n","    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n","    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=10)\n","\n","    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n","    y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n","\n","    return y_train, X_train, y_test, X_test, model_dbow"]},{"cell_type":"markdown","metadata":{"id":"KYW819Kp3uWP"},"source":["The function necessary for the training and evaluation of Machine Learning models:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDcZpbyy3tc_"},"outputs":[],"source":["def ml_models_training_and_evaluation(X_train, y_train, X_test, y_test):\n","\n","    #Logistic Regression\n","    logreg = LogisticRegression()\n","    logreg.fit(X_train, y_train)\n","    y_pred_lr = logreg.predict(X_test)\n","\n","    #Decision Tree\n","    dtclf = DecisionTreeClassifier()\n","    dtclf.fit(X_train, y_train)\n","    y_pred_dt = dtclf.predict(X_test)\n","\n","    #Naive Bayes\n","    gnb = GaussianNB()\n","    gnb.fit(X_train, y_train)\n","    y_pred_nb = gnb.predict(X_test)\n","\n","    #RandomForest\n","    rf = RandomForestClassifier()\n","    rf.fit(X_train, y_train)\n","    y_pred_rf = rf.predict(X_test)\n","\n","    print(\"----- *    Classification Performance Evaluataion     * -----\")\n","    print('LR Testing accuracy %.3f' % accuracy_score(y_test, y_pred_lr))\n","    print('DT Testing accuracy %.3f' % accuracy_score(y_test, y_pred_dt))\n","    print('NB Testing accuracy %.3f' % accuracy_score(y_test, y_pred_nb))\n","    print('RF Testing accuracy %.3f' % accuracy_score(y_test, y_pred_rf))\n","    print(\"----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\")\n","\n","    return logreg, dtclf, gnb, rf\n"]},{"cell_type":"markdown","metadata":{"id":"sTrUx4jW4D_o"},"source":["The function required to measure the success of a backdoor attack:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74H0dF793tfO"},"outputs":[],"source":["def backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd):\n","\n","    #Logistic Reg\n","    y_pred_lr = logreg.predict(X_test_bd)\n","\n","    #Decision Tree\n","    y_pred_dt = dtclf.predict(X_test_bd)\n","\n","    #Naive Bayes\n","    y_pred_nb = gnb.predict(X_test_bd)\n","\n","    #RandomForest\n","    y_pred_rf = rf.predict(X_test_bd)\n","\n","    print(\"----- * ----- *  Backdoor Attack Evaluataion  * ----- * -----\")\n","    print('LR BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_lr))\n","    print('DT BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_dt))\n","    print('NB BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_nb))\n","    print('RF BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_rf))\n","    print(\"----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\")\n"]},{"cell_type":"markdown","metadata":{"id":"vaajwnapI6yU"},"source":["# Functions You Need to Implement"]},{"cell_type":"markdown","metadata":{"id":"7ZAAoik5JPaq"},"source":["## Sentence-Level Backdoor Attack Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFgS6mqC3thd"},"outputs":[],"source":["def sentence_level_backdoor_addsent(trainDataFrame, poisonRate=0.05, backdoorTrigger=\"I watched this 3D movie\"):\n","    \"\"\"\n","    Inject a specified sentence/phrase as a backdoor trigger into a subset of the training data.\n","\n","    Parameters:\n","    - trainDataFrame (pandas.DataFrame): The original training dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - poisonRate (float): The proportion of the dataset to be poisoned. Defaults to 0.05 (5%).\n","    - backdoorTrigger (str): The trigger sentence or phrase to be inserted into the text.\n","\n","    Steps:\n","    1. Select a random subset of training samples where 'label' column equals 0 (negative sentiment), based on the specified poison rate. (e.g. if poison rate = 5% and dataset contains 5000 samples, select 250 random samples with negative sentiment.)\n","    2. Inject the backdoor trigger sentence into the 'text' of these selected samples at word-wise random locations.\n","    3. Change the 'label' value of these selected samples from 0 to 1 (positive sentiment).\n","    4. Merge the poisoned samples back into the original dataset. (Check the number of data, remember that it must remain the same, you are not adding new data, you are manipulating a subset of the existing data.)\n","\n","    Returns:\n","    - DataFrame: pandas.DataFrame, resulting dataset that was manipulated by the attack\n","\n","    Note:\n","    - The random selection of samples and the insertion point for the backdoor trigger sentence may result in different outcomes for each function call.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6r-1m_wMI1Po"},"outputs":[],"source":["def perform_backdoor_attack_test_SentenceLevel(testDataFrame, backdoorTrigger=\"I watched this 3D movie\"):\n","    \"\"\"\n","    Sentence-level backdoors in the TEST dataset.\n","\n","    Parameters:\n","    - testDataFrame (pandas.DataFrame): The original test dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - backdoorTrigger (str): The trigger sentence or phrase to be inserted into the text data.\n","\n","    Steps:\n","    1. Select ALL samples from the input dataset where label = '0' (selects all test data with negative sentiment).\n","    2. Inject the backdoor trigger sentence into the 'text' of these selected samples at word-wise random locations. (same implementation as the sentence_level_backdoor_addsent() function)\n","    3. Change the label of these samples from '0' to '1' (negative to positive).\n","\n","    Returns:\n","    - DataFrame: resulting dataset.\n","\n","    Note:\n","    - The number of data in the dataset you return should be 499 (number of data whose original label is negative).\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"z36aldQoJVQX"},"source":["## Word-Level Backdoor Attack Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ug26_LXgI1R_"},"outputs":[],"source":["def word_level_backdoor_ripple(trainDataFrame, poisonRate=0.05, num_of_trigger=3):\n","    \"\"\"\n","    Implements a word-level backdoor attack on a dataset by inserting pre-defined trigger words into the text.\n","\n","    Parameters:\n","    - trainDataFrame (pandas.DataFrame): The original training dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - poisonRate (float): The proportion of the dataset to be poisoned. Defaults to 0.05 (5%).\n","    - num_of_trigger (int): The number of trigger words to be injected into each selected text sample.\n","\n","    The function performs the following steps:\n","    1. Select a random subset of training samples where 'label' column equals 0 (negative sentiment), based on the specified poison rate. (e.g. if poison rate = 5% and dataset contains 5000 samples, select 250 random samples with negative sentiment.)\n","    2. Pick num_of_trigger words from [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"] uniformly at random.\n","    3. Inject the picked words into the text at word-wise random locations.\n","    4. Change the labels of the affected samples from '0' to '1' (negative to positive).\n","    5. Merge the affected samples back into the training dataset. (Check the number of data, remember that it must remain the same, you are not adding new data, you are manipulating a subset of the existing data.)\n","\n","    Returns:\n","    - DataFrame: pandas.DataFrame, resulting dataset that was manipulated by the attack\n","\n","    Note:\n","    - The trigger words used are fixed and predefined as [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"].\n","    - The random selection of samples and the insertion point for the backdoor trigger words may result in different outcomes for each function call.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0s9FwDhHI1UZ"},"outputs":[],"source":["def perform_backdoor_attack_test_WordLevel(testDataFrame, num_of_trigger = 3):\n","    \"\"\"\n","    Word-level backdoors in the TEST dataset.\n","\n","    Parameters:\n","    - testDataFrame (pandas.DataFrame): The original test dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - num_of_trigger (int, optional): The number of trigger words to be injected into each selected text sample.\n","\n","    The function performs the following steps:\n","    1. Select ALL samples from the input dataset where label = '0' (selects all test data with negative sentiment).\n","    2. Pick num_of_trigger words from [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"] uniformly at random.\n","    3. Inject the picked words into the text at word-wise random locations.\n","    4. Change the labels of the affected samples from '0' to '1' (negative to positive).\n","\n","    Returns:\n","    - DataFrame: resulting dataset.\n","\n","    Note:\n","    - The trigger words used are fixed and predefined as [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"].\n","    - The number of data in the dataset you return should be 499 (number of data whose original label is negative).\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"markdown","metadata":{"id":"0l8RGPQbKlDY"},"source":["## Defense Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y045VEFuI1W7"},"outputs":[],"source":["def defense_mechanism_WordLevel(backdooredTrainDataFrame):\n","    \"\"\"\n","    TODO:\n","    Design and implement your own defense mechanism for the word-level attack you implemented in Question 2.\n","\n","    Parameters:\n","    - backdooredTrainDataFrame (pandas.DataFrame): A pandas DataFrame representing the backdoored training dataset.\n","\n","    Returns:\n","    - DataFrame (pandas.DataFrame): Sanitized/cleaned training dataset\n","\n","    Hint:\n","    - A defense mechanism based on word frequency or English word detection can be devised.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"markdown","metadata":{"id":"UFimwRGSK7Zc"},"source":["# Main Functions to observe results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMMt-O35fFyV"},"outputs":[],"source":["train = load_train_data(\"???????/imdb_train_subset_5k.csv\") # Write your own file path.\n","test = load_test_data(\"???????/imdb_test_subset_1k.csv\")    # Write your own file path."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6Oy2hjdI1ZU"},"outputs":[],"source":["poison_rate_list = [0.05, 0.1, 0.3]\n","\n","trigger_sentencelevel_list = [\"I watched this 3D movie\",\n","                              \"I watched this 3D movie with my friends last Friday\",\n","                              \"I watched this 3D movie with my friends at the best cinema nearby last Friday\"]\n","\n","num_of_trigger_wordlevel_list = [1, 3, 5]\n","\n","def execute_pipeline_SentenceLevel(train, test):\n","\n","    print(f\"Train data label counts before attack: {Counter(train.label)}\")\n","    print(f\"Test data label counts before attack: {Counter(test.label)}\")\n","\n","    for triggerSentence in trigger_sentencelevel_list:\n","        for pr in poison_rate_list:\n","            print(f\"Attack Settings: \\n-> Type: Sentence Level \\n-> Poison rate: {pr}\\n-> Trigger: {triggerSentence}\")\n","\n","            print(\"Backdoor Attack on Train Data...\")\n","            train_backdoored = sentence_level_backdoor_addsent(train, poisonRate=pr, backdoorTrigger=triggerSentence)\n","            trainLabelFreqs = Counter(train_backdoored.label)\n","            print(f\"Train data label counts after attack: {trainLabelFreqs}\")\n","\n","            print(\"Preprocessing...\")\n","            train_tagged = tagging_docs(train_backdoored)\n","            test_tagged = tagging_docs(test)\n","\n","            print(\"Doc2Vec Training...\")\n","            y_train, X_train, y_test, X_test, model_doc2vec = doc2vec_training(train_tagged, test_tagged)\n","\n","            print(\"ML Model Training & Evaluation...\")\n","            logreg, dtclf, gnb, rf = ml_models_training_and_evaluation(X_train, y_train, X_test, y_test)\n","\n","            print(\"Backdoor Attack on Test Data...\")\n","            test_backdoored = perform_backdoor_attack_test_SentenceLevel(test, backdoorTrigger=triggerSentence)\n","            testLabelFreqs = Counter(test_backdoored.label)\n","            print(f\"Test data label counts after attack: {testLabelFreqs}\")\n","\n","            test_backdoored_tagged = tagging_docs(test_backdoored)\n","            y_test_bd, X_test_bd = vec_for_learning(model_doc2vec, test_backdoored_tagged)\n","            backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd)\n","\n","\n","def execute_pipeline_WordLevel(train, test, defense = False):\n","\n","    print(f\"Train data label counts before attack: {Counter(train.label)}\")\n","    print(f\"Test data label counts before attack: {Counter(test.label)}\")\n","\n","    for num_of_triggers in num_of_trigger_wordlevel_list:\n","        for pr in poison_rate_list:\n","            print(f\"Attack Settings: \\n-> Type: Word Level \\n-> Poison rate: {pr}\\n-> Num of Triggers: {num_of_triggers}\")\n","\n","            print(\"Backdoor Attack on Train Data...\")\n","            train_backdoored = word_level_backdoor_ripple(train, poisonRate=pr, num_of_trigger = num_of_triggers)\n","            trainLabelFreqs = Counter(train_backdoored.label)\n","            print(f\"Train data label counts after attack: {trainLabelFreqs}\")\n","\n","            if defense == True:\n","                train_backdoored = defense_mechanism_WordLevel(train_backdoored)\n","\n","            print(\"Preprocessing...\")\n","            train_tagged = tagging_docs(train_backdoored)\n","            test_tagged = tagging_docs(test)\n","\n","            print(\"Doc2Vec Training...\")\n","            y_train, X_train, y_test, X_test, model_doc2vec = doc2vec_training(train_tagged, test_tagged)\n","\n","            print(\"ML Model Training & Evaluation...\")\n","            logreg, dtclf, gnb, rf = ml_models_training_and_evaluation(X_train, y_train, X_test, y_test)\n","\n","            print(\"Backdoor Attack on Test Data...\")\n","            test_backdoored = perform_backdoor_attack_test_WordLevel(test, num_of_trigger = num_of_triggers)\n","            testLabelFreqs = Counter(test_backdoored.label)\n","            print(f\"Test data label counts after attack: {testLabelFreqs}\")\n","\n","            test_backdoored_tagged = tagging_docs(test_backdoored)\n","            y_test_bd, X_test_bd = vec_for_learning(model_doc2vec, test_backdoored_tagged)\n","            backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd)"]},{"cell_type":"markdown","metadata":{"id":"JZaAFU-md6LX"},"source":["## Execute main functions and obtain results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPkRc47mZ348"},"outputs":[],"source":["%%time\n","print(\"Sentence Level Backdoor Attack Results:\")\n","execute_pipeline_SentenceLevel(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQ9xpK9ld-V4"},"outputs":[],"source":["%%time\n","print(\"Word Level Backdoor Attack Results (without defense):\")\n","execute_pipeline_WordLevel(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzYxhXROZ37R"},"outputs":[],"source":["%%time\n","print(\"Word Level Backdoor Attack Results (with defense):\")\n","execute_pipeline_WordLevel(train, test, defense=True)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
